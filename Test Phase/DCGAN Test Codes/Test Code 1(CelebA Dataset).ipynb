{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRo3I8pTXMgu","outputId":"8a90f225-3c84-4eda-bd1d-d2b6950fa0de","executionInfo":{"status":"ok","timestamp":1734275380315,"user_tz":-330,"elapsed":64696,"user":{"displayName":"Deep Habiswashi","userId":"14306398219012041066"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.4)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n","Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n","Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/celeba-dataset?dataset_version_number=2...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.33G/1.33G [00:18<00:00, 75.3MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2\n"]}],"source":["#Setup and Dataset Downloading\n","\n","# Install necessary packages\n","!pip install kagglehub tensorflow pillow pandas matplotlib\n","\n","# Import KaggleHub to download the dataset\n","import kagglehub\n","\n","# Download the CelebA dataset\n","path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n","print(\"Path to dataset files:\", path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyD3zzILXn_p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734275417840,"user_tz":-330,"elapsed":31948,"user":{"displayName":"Deep Habiswashi","userId":"14306398219012041066"}},"outputId":"32282a13-252d-448f-ffac-1dff2ab4d2a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 10000 smiling images.\n"]}],"source":["# Dataset Path and Preprocessing\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Paths to dataset files (assuming default paths after KaggleHub download)\n","data_dir = os.path.join(path, 'img_align_celeba/img_align_celeba')\n","attr_path = os.path.join(path, 'list_attr_celeba.csv')\n","partition_path = os.path.join(path, 'list_eval_partition.csv')\n","\n","img_size = 128\n","\n","# Load attributes and partition files\n","attributes = pd.read_csv(attr_path)\n","partition = pd.read_csv(partition_path)\n","\n","# Preprocess images (filter for smiling faces)\n","smiling_images = attributes[attributes['Smiling'] == 1]['image_id']\n","image_paths = smiling_images.map(lambda x: os.path.join(data_dir, x))\n","\n","def preprocess_image(image_path):\n","    img = Image.open(image_path).resize((img_size, img_size))\n","    img = np.asarray(img) / 255.0  # Normalize to [0, 1]\n","    return img\n","\n","images = np.array([preprocess_image(img) for img in image_paths[:10000]])\n","print(f\"Loaded {len(images)} smiling images.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAmyHbd7YALD"},"outputs":[],"source":["#Building the Generator\n","\n","from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, LeakyReLU, BatchNormalization\n","\n","def build_generator():\n","    model = tf.keras.Sequential([\n","        Dense(8 * 8 * 256, input_dim=100, activation='relu'),\n","        Reshape((8, 8, 256)),\n","        BatchNormalization(),\n","        LeakyReLU(0.2),\n","\n","        Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same'),\n","        BatchNormalization(),\n","        LeakyReLU(0.2),\n","\n","        Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'),\n","        BatchNormalization(),\n","        LeakyReLU(0.2),\n","\n","        Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh')\n","    ])\n","    return model\n","   model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o__cFNLnYGi0"},"outputs":[],"source":["#Building the Discriminator\n","\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout\n","\n","def build_discriminator():\n","    model = tf.keras.Sequential([\n","        Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(64, 64, 3)),\n","        LeakyReLU(0.2),\n","        Dropout(0.3),\n","\n","        Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n","        LeakyReLU(0.2),\n","        Dropout(0.3),\n","\n","        Flatten(),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","    model.summary()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBaxJQSfZOCi"},"outputs":[],"source":["# Preprocessing the Dataset Correctly\n","\n","img_size = 64  # Ensure generator and discriminator work with 64x64\n","\n","def preprocess_image(image_path):\n","    img = Image.open(image_path).resize((img_size, img_size))  # Resize to 64x64\n","    img = np.asarray(img) / 127.5 - 1.0  # Normalize to [-1, 1] for tanh activation\n","    return img\n","\n","images = np.array([preprocess_image(img) for img in image_paths[:10000]])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7IQ40cFYMRx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734275450093,"user_tz":-330,"elapsed":3375,"user":{"displayName":"Deep Habiswashi","userId":"14306398219012041066"}},"outputId":"9a17a0d2-e23c-4bf7-d3f0-442ad9973ff5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["# Training the GAN-Loss Functions and Optimizers\n","\n","cross_entropy = tf.keras.losses.BinaryCrossentropy()\n","\n","generator = build_generator()\n","discriminator = build_discriminator()\n","\n","generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    return real_loss + fake_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HCW9knhYVBZ","outputId":"1d8d3e68-889b-4462-ea2b-d86b56a13c53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/300 | Gen Loss: 2.7100 | Disc Loss: 0.4337\n","Epoch 2/300 | Gen Loss: 1.7575 | Disc Loss: 0.8610\n","Epoch 3/300 | Gen Loss: 1.4510 | Disc Loss: 0.8548\n","Epoch 4/300 | Gen Loss: 1.5672 | Disc Loss: 0.9061\n","Epoch 5/300 | Gen Loss: 1.4112 | Disc Loss: 0.8851\n","Epoch 6/300 | Gen Loss: 1.4192 | Disc Loss: 0.8933\n","Epoch 7/300 | Gen Loss: 1.3590 | Disc Loss: 0.9062\n","Epoch 8/300 | Gen Loss: 1.4142 | Disc Loss: 0.8984\n","Epoch 9/300 | Gen Loss: 1.4583 | Disc Loss: 0.8275\n","Epoch 10/300 | Gen Loss: 1.5295 | Disc Loss: 0.7968\n","Epoch 11/300 | Gen Loss: 1.5454 | Disc Loss: 0.8283\n","Epoch 12/300 | Gen Loss: 1.5062 | Disc Loss: 0.8685\n","Epoch 13/300 | Gen Loss: 1.5778 | Disc Loss: 0.8351\n","Epoch 14/300 | Gen Loss: 1.5992 | Disc Loss: 0.8464\n","Epoch 15/300 | Gen Loss: 1.5497 | Disc Loss: 0.8700\n","Epoch 16/300 | Gen Loss: 1.5732 | Disc Loss: 0.8366\n","Epoch 17/300 | Gen Loss: 1.6722 | Disc Loss: 0.7851\n","Epoch 18/300 | Gen Loss: 1.6911 | Disc Loss: 0.7923\n","Epoch 19/300 | Gen Loss: 1.8205 | Disc Loss: 0.7311\n","Epoch 20/300 | Gen Loss: 1.7166 | Disc Loss: 0.7952\n","Epoch 21/300 | Gen Loss: 1.8054 | Disc Loss: 0.7540\n","Epoch 22/300 | Gen Loss: 1.7178 | Disc Loss: 0.8000\n","Epoch 23/300 | Gen Loss: 1.8108 | Disc Loss: 0.7619\n","Epoch 24/300 | Gen Loss: 1.7889 | Disc Loss: 0.7790\n","Epoch 25/300 | Gen Loss: 1.7363 | Disc Loss: 0.7824\n","Epoch 26/300 | Gen Loss: 1.7339 | Disc Loss: 0.8072\n","Epoch 27/300 | Gen Loss: 1.7208 | Disc Loss: 0.8153\n","Epoch 28/300 | Gen Loss: 1.6381 | Disc Loss: 0.8557\n","Epoch 29/300 | Gen Loss: 1.6402 | Disc Loss: 0.8568\n","Epoch 30/300 | Gen Loss: 1.5499 | Disc Loss: 0.9236\n","Epoch 31/300 | Gen Loss: 1.5183 | Disc Loss: 0.9486\n","Epoch 32/300 | Gen Loss: 1.4712 | Disc Loss: 0.9747\n","Epoch 33/300 | Gen Loss: 1.4302 | Disc Loss: 0.9972\n","Epoch 34/300 | Gen Loss: 1.4216 | Disc Loss: 0.9922\n","Epoch 35/300 | Gen Loss: 1.3336 | Disc Loss: 1.0546\n","Epoch 36/300 | Gen Loss: 1.3390 | Disc Loss: 1.0280\n","Epoch 37/300 | Gen Loss: 1.3489 | Disc Loss: 1.0510\n","Epoch 38/300 | Gen Loss: 1.2618 | Disc Loss: 1.0795\n","Epoch 39/300 | Gen Loss: 1.2465 | Disc Loss: 1.0946\n","Epoch 40/300 | Gen Loss: 1.2314 | Disc Loss: 1.0997\n","Epoch 41/300 | Gen Loss: 1.2177 | Disc Loss: 1.1033\n","Epoch 42/300 | Gen Loss: 1.1911 | Disc Loss: 1.1203\n","Epoch 43/300 | Gen Loss: 1.1913 | Disc Loss: 1.1252\n","Epoch 44/300 | Gen Loss: 1.1785 | Disc Loss: 1.1251\n","Epoch 45/300 | Gen Loss: 1.1544 | Disc Loss: 1.1190\n","Epoch 46/300 | Gen Loss: 1.1618 | Disc Loss: 1.1225\n","Epoch 47/300 | Gen Loss: 1.1368 | Disc Loss: 1.1502\n","Epoch 48/300 | Gen Loss: 1.1040 | Disc Loss: 1.1508\n","Epoch 49/300 | Gen Loss: 1.1001 | Disc Loss: 1.1646\n","Epoch 50/300 | Gen Loss: 1.1100 | Disc Loss: 1.1537\n","Epoch 51/300 | Gen Loss: 1.1121 | Disc Loss: 1.1558\n","Epoch 52/300 | Gen Loss: 1.0944 | Disc Loss: 1.1484\n","Epoch 53/300 | Gen Loss: 1.1201 | Disc Loss: 1.1521\n","Epoch 54/300 | Gen Loss: 1.0917 | Disc Loss: 1.1600\n","Epoch 55/300 | Gen Loss: 1.0823 | Disc Loss: 1.1759\n","Epoch 56/300 | Gen Loss: 1.0830 | Disc Loss: 1.1674\n","Epoch 57/300 | Gen Loss: 1.0809 | Disc Loss: 1.1579\n","Epoch 58/300 | Gen Loss: 1.1226 | Disc Loss: 1.1362\n","Epoch 59/300 | Gen Loss: 1.1032 | Disc Loss: 1.1490\n","Epoch 60/300 | Gen Loss: 1.1141 | Disc Loss: 1.1489\n","Epoch 61/300 | Gen Loss: 1.1174 | Disc Loss: 1.1469\n","Epoch 62/300 | Gen Loss: 1.1050 | Disc Loss: 1.1509\n","Epoch 63/300 | Gen Loss: 1.1275 | Disc Loss: 1.1354\n","Epoch 64/300 | Gen Loss: 1.0718 | Disc Loss: 1.1815\n","Epoch 65/300 | Gen Loss: 1.1073 | Disc Loss: 1.1322\n","Epoch 66/300 | Gen Loss: 1.0947 | Disc Loss: 1.1656\n","Epoch 67/300 | Gen Loss: 1.0667 | Disc Loss: 1.1704\n","Epoch 68/300 | Gen Loss: 1.0780 | Disc Loss: 1.1551\n","Epoch 69/300 | Gen Loss: 1.1144 | Disc Loss: 1.1371\n","Epoch 70/300 | Gen Loss: 1.1063 | Disc Loss: 1.1576\n","Epoch 71/300 | Gen Loss: 1.0873 | Disc Loss: 1.1503\n","Epoch 72/300 | Gen Loss: 1.1043 | Disc Loss: 1.1505\n","Epoch 73/300 | Gen Loss: 1.1188 | Disc Loss: 1.1249\n","Epoch 74/300 | Gen Loss: 1.1299 | Disc Loss: 1.1237\n","Epoch 75/300 | Gen Loss: 1.1299 | Disc Loss: 1.1272\n","Epoch 76/300 | Gen Loss: 1.1507 | Disc Loss: 1.1327\n","Epoch 77/300 | Gen Loss: 1.1385 | Disc Loss: 1.1418\n","Epoch 78/300 | Gen Loss: 1.1255 | Disc Loss: 1.1300\n","Epoch 79/300 | Gen Loss: 1.1152 | Disc Loss: 1.1349\n","Epoch 80/300 | Gen Loss: 1.1309 | Disc Loss: 1.1357\n","Epoch 81/300 | Gen Loss: 1.1452 | Disc Loss: 1.1180\n","Epoch 82/300 | Gen Loss: 1.1056 | Disc Loss: 1.1362\n","Epoch 83/300 | Gen Loss: 1.1335 | Disc Loss: 1.1251\n","Epoch 84/300 | Gen Loss: 1.1604 | Disc Loss: 1.1003\n","Epoch 85/300 | Gen Loss: 1.1586 | Disc Loss: 1.1271\n","Epoch 86/300 | Gen Loss: 1.1521 | Disc Loss: 1.1273\n","Epoch 87/300 | Gen Loss: 1.1299 | Disc Loss: 1.1329\n","Epoch 88/300 | Gen Loss: 1.1055 | Disc Loss: 1.1465\n","Epoch 89/300 | Gen Loss: 1.1139 | Disc Loss: 1.1195\n","Epoch 90/300 | Gen Loss: 1.1706 | Disc Loss: 1.1101\n","Epoch 91/300 | Gen Loss: 1.1474 | Disc Loss: 1.1228\n","Epoch 92/300 | Gen Loss: 1.1418 | Disc Loss: 1.1184\n","Epoch 93/300 | Gen Loss: 1.1262 | Disc Loss: 1.1493\n","Epoch 94/300 | Gen Loss: 1.1644 | Disc Loss: 1.1129\n","Epoch 95/300 | Gen Loss: 1.1779 | Disc Loss: 1.1053\n","Epoch 96/300 | Gen Loss: 1.1555 | Disc Loss: 1.1253\n","Epoch 97/300 | Gen Loss: 1.1817 | Disc Loss: 1.0792\n","Epoch 98/300 | Gen Loss: 1.1602 | Disc Loss: 1.1203\n","Epoch 99/300 | Gen Loss: 1.1579 | Disc Loss: 1.1245\n","Epoch 100/300 | Gen Loss: 1.1906 | Disc Loss: 1.0933\n","Epoch 101/300 | Gen Loss: 1.1658 | Disc Loss: 1.1161\n","Epoch 102/300 | Gen Loss: 1.1572 | Disc Loss: 1.1107\n","Epoch 103/300 | Gen Loss: 1.1478 | Disc Loss: 1.1160\n","Epoch 104/300 | Gen Loss: 1.1615 | Disc Loss: 1.1285\n","Epoch 105/300 | Gen Loss: 1.1601 | Disc Loss: 1.1246\n","Epoch 106/300 | Gen Loss: 1.1250 | Disc Loss: 1.1333\n","Epoch 107/300 | Gen Loss: 1.1957 | Disc Loss: 1.1059\n","Epoch 108/300 | Gen Loss: 1.1788 | Disc Loss: 1.0857\n","Epoch 109/300 | Gen Loss: 1.2108 | Disc Loss: 1.1257\n","Epoch 110/300 | Gen Loss: 1.1824 | Disc Loss: 1.0928\n","Epoch 111/300 | Gen Loss: 1.1657 | Disc Loss: 1.1123\n","Epoch 112/300 | Gen Loss: 1.1606 | Disc Loss: 1.1162\n","Epoch 113/300 | Gen Loss: 1.1615 | Disc Loss: 1.1070\n","Epoch 114/300 | Gen Loss: 1.2075 | Disc Loss: 1.0904\n","Epoch 115/300 | Gen Loss: 1.1855 | Disc Loss: 1.0841\n","Epoch 116/300 | Gen Loss: 1.1923 | Disc Loss: 1.0947\n","Epoch 117/300 | Gen Loss: 1.1845 | Disc Loss: 1.1141\n","Epoch 118/300 | Gen Loss: 1.1664 | Disc Loss: 1.1139\n","Epoch 119/300 | Gen Loss: 1.1865 | Disc Loss: 1.0845\n","Epoch 120/300 | Gen Loss: 1.1807 | Disc Loss: 1.1017\n","Epoch 121/300 | Gen Loss: 1.2354 | Disc Loss: 1.1034\n","Epoch 122/300 | Gen Loss: 1.2313 | Disc Loss: 1.0941\n","Epoch 123/300 | Gen Loss: 1.1773 | Disc Loss: 1.0977\n","Epoch 124/300 | Gen Loss: 1.1582 | Disc Loss: 1.1219\n","Epoch 125/300 | Gen Loss: 1.1949 | Disc Loss: 1.0933\n","Epoch 126/300 | Gen Loss: 1.2029 | Disc Loss: 1.1066\n","Epoch 127/300 | Gen Loss: 1.1918 | Disc Loss: 1.0874\n","Epoch 128/300 | Gen Loss: 1.2060 | Disc Loss: 1.0869\n","Epoch 129/300 | Gen Loss: 1.2006 | Disc Loss: 1.0938\n","Epoch 130/300 | Gen Loss: 1.2067 | Disc Loss: 1.0766\n","Epoch 131/300 | Gen Loss: 1.2248 | Disc Loss: 1.1126\n","Epoch 132/300 | Gen Loss: 1.1914 | Disc Loss: 1.0907\n","Epoch 133/300 | Gen Loss: 1.2173 | Disc Loss: 1.1067\n","Epoch 134/300 | Gen Loss: 1.2119 | Disc Loss: 1.0879\n","Epoch 135/300 | Gen Loss: 1.2066 | Disc Loss: 1.0979\n","Epoch 136/300 | Gen Loss: 1.2029 | Disc Loss: 1.0939\n","Epoch 137/300 | Gen Loss: 1.2221 | Disc Loss: 1.0898\n","Epoch 138/300 | Gen Loss: 1.2100 | Disc Loss: 1.1185\n","Epoch 139/300 | Gen Loss: 1.2210 | Disc Loss: 1.0629\n","Epoch 140/300 | Gen Loss: 1.2020 | Disc Loss: 1.1193\n","Epoch 141/300 | Gen Loss: 1.2054 | Disc Loss: 1.1025\n","Epoch 142/300 | Gen Loss: 1.1864 | Disc Loss: 1.0759\n","Epoch 143/300 | Gen Loss: 1.2054 | Disc Loss: 1.1017\n","Epoch 144/300 | Gen Loss: 1.2807 | Disc Loss: 1.0358\n","Epoch 145/300 | Gen Loss: 1.2360 | Disc Loss: 1.0769\n","Epoch 146/300 | Gen Loss: 1.2402 | Disc Loss: 1.1260\n","Epoch 147/300 | Gen Loss: 1.2202 | Disc Loss: 1.1013\n","Epoch 148/300 | Gen Loss: 1.2555 | Disc Loss: 1.0726\n","Epoch 149/300 | Gen Loss: 1.2145 | Disc Loss: 1.1142\n","Epoch 150/300 | Gen Loss: 1.2141 | Disc Loss: 1.0886\n","Epoch 151/300 | Gen Loss: 1.2285 | Disc Loss: 1.0872\n","Epoch 152/300 | Gen Loss: 1.2463 | Disc Loss: 1.0856\n","Epoch 153/300 | Gen Loss: 1.2081 | Disc Loss: 1.1215\n","Epoch 154/300 | Gen Loss: 1.2134 | Disc Loss: 1.0795\n","Epoch 155/300 | Gen Loss: 1.2000 | Disc Loss: 1.0984\n","Epoch 156/300 | Gen Loss: 1.2209 | Disc Loss: 1.1165\n","Epoch 157/300 | Gen Loss: 1.2319 | Disc Loss: 1.0738\n","Epoch 158/300 | Gen Loss: 1.2471 | Disc Loss: 1.1129\n","Epoch 159/300 | Gen Loss: 1.1924 | Disc Loss: 1.0797\n","Epoch 160/300 | Gen Loss: 1.2443 | Disc Loss: 1.0870\n","Epoch 161/300 | Gen Loss: 1.2296 | Disc Loss: 1.0962\n","Epoch 162/300 | Gen Loss: 1.2440 | Disc Loss: 1.0749\n","Epoch 163/300 | Gen Loss: 1.2550 | Disc Loss: 1.1029\n","Epoch 164/300 | Gen Loss: 1.2583 | Disc Loss: 1.0815\n","Epoch 165/300 | Gen Loss: 1.1838 | Disc Loss: 1.1061\n","Epoch 166/300 | Gen Loss: 1.2109 | Disc Loss: 1.0712\n","Epoch 167/300 | Gen Loss: 1.2340 | Disc Loss: 1.0915\n","Epoch 168/300 | Gen Loss: 1.2394 | Disc Loss: 1.0721\n","Epoch 169/300 | Gen Loss: 1.2478 | Disc Loss: 1.1055\n","Epoch 170/300 | Gen Loss: 1.2674 | Disc Loss: 1.0746\n","Epoch 171/300 | Gen Loss: 1.2435 | Disc Loss: 1.0721\n","Epoch 172/300 | Gen Loss: 1.2451 | Disc Loss: 1.0768\n","Epoch 173/300 | Gen Loss: 1.2385 | Disc Loss: 1.0832\n","Epoch 174/300 | Gen Loss: 1.2500 | Disc Loss: 1.0637\n","Epoch 175/300 | Gen Loss: 1.2287 | Disc Loss: 1.0839\n","Epoch 176/300 | Gen Loss: 1.2400 | Disc Loss: 1.0596\n","Epoch 177/300 | Gen Loss: 1.2388 | Disc Loss: 1.0781\n","Epoch 178/300 | Gen Loss: 1.2760 | Disc Loss: 1.0469\n","Epoch 179/300 | Gen Loss: 1.2617 | Disc Loss: 1.0740\n","Epoch 180/300 | Gen Loss: 1.2466 | Disc Loss: 1.0852\n","Epoch 181/300 | Gen Loss: 1.2506 | Disc Loss: 1.0643\n","Epoch 182/300 | Gen Loss: 1.2756 | Disc Loss: 1.0689\n","Epoch 183/300 | Gen Loss: 1.2490 | Disc Loss: 1.0721\n","Epoch 184/300 | Gen Loss: 1.2756 | Disc Loss: 1.1107\n","Epoch 185/300 | Gen Loss: 1.2815 | Disc Loss: 1.0824\n","Epoch 186/300 | Gen Loss: 1.2889 | Disc Loss: 1.0677\n","Epoch 187/300 | Gen Loss: 1.2434 | Disc Loss: 1.0828\n","Epoch 188/300 | Gen Loss: 1.2851 | Disc Loss: 1.0512\n","Epoch 189/300 | Gen Loss: 1.2319 | Disc Loss: 1.0490\n","Epoch 190/300 | Gen Loss: 1.3552 | Disc Loss: 1.0197\n","Epoch 191/300 | Gen Loss: 1.2928 | Disc Loss: 1.0472\n","Epoch 192/300 | Gen Loss: 1.2871 | Disc Loss: 1.0487\n","Epoch 193/300 | Gen Loss: 1.2717 | Disc Loss: 1.0552\n","Epoch 194/300 | Gen Loss: 1.2495 | Disc Loss: 1.0604\n","Epoch 195/300 | Gen Loss: 1.2722 | Disc Loss: 1.0608\n","Epoch 196/300 | Gen Loss: 1.3253 | Disc Loss: 1.0512\n","Epoch 197/300 | Gen Loss: 1.2724 | Disc Loss: 1.0367\n","Epoch 198/300 | Gen Loss: 1.2610 | Disc Loss: 1.0674\n","Epoch 199/300 | Gen Loss: 1.2695 | Disc Loss: 1.0560\n","Epoch 200/300 | Gen Loss: 1.3024 | Disc Loss: 1.0322\n","Epoch 201/300 | Gen Loss: 1.2960 | Disc Loss: 1.0765\n","Epoch 202/300 | Gen Loss: 1.3032 | Disc Loss: 1.0657\n","Epoch 203/300 | Gen Loss: 1.2841 | Disc Loss: 1.0799\n","Epoch 204/300 | Gen Loss: 1.3034 | Disc Loss: 1.0336\n","Epoch 205/300 | Gen Loss: 1.3506 | Disc Loss: 1.0503\n","Epoch 206/300 | Gen Loss: 1.2744 | Disc Loss: 1.0581\n","Epoch 207/300 | Gen Loss: 1.2854 | Disc Loss: 1.0522\n","Epoch 208/300 | Gen Loss: 1.3188 | Disc Loss: 1.0562\n","Epoch 209/300 | Gen Loss: 1.2959 | Disc Loss: 1.0409\n","Epoch 210/300 | Gen Loss: 1.2989 | Disc Loss: 1.0460\n","Epoch 211/300 | Gen Loss: 1.3029 | Disc Loss: 1.0502\n"]}],"source":["# Training Loop\n","\n","@tf.function\n","def train_step(real_images):\n","    noise = tf.random.normal([BATCH_SIZE, 100])\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training=True)\n","        real_output = discriminator(real_images, training=True)\n","        fake_output = discriminator(generated_images, training=True)\n","\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n","\n","    return gen_loss, disc_loss\n","\n","EPOCHS = 300\n","BATCH_SIZE = 16\n","\n","for epoch in range(EPOCHS):\n","    gen_loss_epoch, disc_loss_epoch = [], []\n","    for i in range(0, len(images), BATCH_SIZE):\n","        batch = images[i:i + BATCH_SIZE]\n","        if len(batch) == BATCH_SIZE:\n","            gen_loss, disc_loss = train_step(batch)\n","            gen_loss_epoch.append(gen_loss.numpy())\n","            disc_loss_epoch.append(disc_loss.numpy())\n","\n","    print(f'Epoch {epoch+1}/{EPOCHS} | Gen Loss: {np.mean(gen_loss_epoch):.4f} | Disc Loss: {np.mean(disc_loss_epoch):.4f}')"]},{"cell_type":"code","source":["# prompt: By analyzing the previous code statement, I want to save and download the model I am training\n","\n","# Save the generator model\n","generator.save('/content/drive/My Drive/generator_model1')\n","\n","# Save the discriminator model\n","discriminator.save('/content/drive/My Drive/discriminator_model1')\n","\n","print(\"Models saved to Google Drive.\")\n","\n","#To download the model:\n","#from google.colab import files\n","#files.download('/content/drive/My Drive/generator_model')\n","#files.download('/content/drive/My Drive/discriminator_model')"],"metadata":{"id":"rA7jCqRh4J3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJIR72U1bPoT"},"outputs":[],"source":["import IPython.display as display\n","from google.colab import files\n","import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Function to process the uploaded image\n","def preprocess_uploaded_image(image_path):\n","    img = Image.open(image_path).convert(\"RGB\")  # Convert to RGB to handle all formats\n","    img = img.resize((64, 64))  # Resize to match the model input\n","    img_array = np.asarray(img) / 127.5 - 1.0  # Normalize to [-1, 1]\n","    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n","    return img_array\n","\n","# Function to generate a smiling face\n","def generate_smiling_face(generator):\n","    print(\"Please upload an image (any format like .jpg, .png, etc.)\")\n","    uploaded = files.upload()  # Upload the image\n","    for image_name in uploaded.keys():\n","        print(f\"Image {image_name} uploaded successfully!\")\n","\n","        # Preprocess the uploaded image\n","        input_img = preprocess_uploaded_image(image_name)\n","\n","        # Generate the smiling face using the generator\n","        noise = tf.random.normal([1, 100])\n","        generated_image = generator(noise, training=False)\n","\n","        # Convert the generated image from [-1, 1] to [0, 1] range for display\n","        generated_image = (generated_image[0] + 1) / 2.0\n","\n","        # Display the original and generated images side by side\n","        plt.figure(figsize=(8, 4))\n","        plt.subplot(1, 2, 1)\n","        plt.title(\"Uploaded Image\")\n","        plt.imshow(np.asarray(Image.open(image_name).resize((64, 64))))\n","        plt.axis('off')\n","\n","        plt.subplot(1, 2, 2)\n","        plt.title(\"Smiling Face\")\n","        plt.imshow(generated_image.numpy())\n","        plt.axis('off')\n","\n","        plt.show()\n","\n","# Run the function to upload and generate a smiling face\n","generate_smiling_face(generator)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7uC4JEDcY56"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}